--- git status ---
On branch main
Your branch is ahead of 'origin/main' by 7 commits.
  (use "git push" to publish your local commits)

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   exts/extensions/extensions/tutorials/manager_based/flat_env_cfg.py
	modified:   exts/extensions/extensions/tutorials/manager_based/play.py
	deleted:    exts/extensions/extensions/tutorials/manager_based/sb3/train_sb3.py
	deleted:    exts/extensions/extensions/tutorials/manager_based/sb4/play.py
	deleted:    exts/extensions/extensions/tutorials/manager_based/sb4/train.py
	modified:   exts/extensions/extensions/tutorials/manager_based/train.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	exts/extensions/extensions/tutorials/manager_based/cfg_agent.yaml

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/exts/extensions/extensions/tutorials/manager_based/flat_env_cfg.py b/exts/extensions/extensions/tutorials/manager_based/flat_env_cfg.py
index b6c1dd9..166611d 100644
--- a/exts/extensions/extensions/tutorials/manager_based/flat_env_cfg.py
+++ b/exts/extensions/extensions/tutorials/manager_based/flat_env_cfg.py
@@ -13,6 +13,11 @@ from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg imp
 from omni.isaac.lab_assets.anymal import ANYMAL_D_CFG  # isort: skip
 from quadruped_env import LocomotionVelocityEnvCfg
 
+import argparse
+parser = argparse.ArgumentParser(description="Tutorial on running the cartpole RL environment.")
+parser.add_argument("--num_envs", type=int, default=16, help="Number of environments to spawn.")
+
+
 @configclass
 class AnymalDFlatEnvCfg(LocomotionVelocityEnvCfg):
     def __post_init__(self):
diff --git a/exts/extensions/extensions/tutorials/manager_based/play.py b/exts/extensions/extensions/tutorials/manager_based/play.py
index 0cdeab7..53efe60 100644
--- a/exts/extensions/extensions/tutorials/manager_based/play.py
+++ b/exts/extensions/extensions/tutorials/manager_based/play.py
@@ -1,16 +1,20 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
+"""
+    This script use for interacting with an articulation for quadruped robot in Isaac Sim.
+
+    #   Usage
+    python exts/extensions/extensions/tutorials/manager_based/play.py --num_envs 1 --checkpoint logs/sb3/AnymalDFlatEnv/2024-11-05_19-19-41/model_1004000_steps.zip
+"""
 
 """Script to play a checkpoint if an RL agent from Stable-Baselines3."""
 
-"""Launch Isaac Sim Simulator first."""
+##############################################################
+#
+#           Setting argparse arguments 
+#
+##############################################################
 
 import argparse
 
-from omni.isaac.lab.app import AppLauncher
-
 # add argparse arguments
 parser = argparse.ArgumentParser(description="Play a checkpoint of an RL agent from Stable-Baselines3.")
 parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
@@ -26,6 +30,15 @@ parser.add_argument(
     action="store_true",
     help="When no checkpoint provided, use the last saved model. Otherwise use the best saved model.",
 )
+
+##############################################################
+#
+#           Launch Isaac Sim Simulator first.
+#
+##############################################################
+
+from omni.isaac.lab.app import AppLauncher
+
 # append AppLauncher cli args
 AppLauncher.add_app_launcher_args(parser)
 # parse the arguments
@@ -38,7 +51,11 @@ if args_cli.video:
 app_launcher = AppLauncher(args_cli)
 simulation_app = app_launcher.app
 
-"""Rest everything follows."""
+##############################################################
+#
+#           Import the library for run play
+#
+##############################################################
 
 import gymnasium as gym
 import numpy as np
@@ -49,228 +66,59 @@ from stable_baselines3 import PPO
 from stable_baselines3.common.vec_env import VecNormalize
 
 from omni.isaac.lab.utils.dict import print_dict
+from omni.isaac.lab.envs import ManagerBasedRLEnv
 
-import omni.isaac.lab_tasks  # noqa: F401
 from omni.isaac.lab_tasks.utils.parse_cfg import get_checkpoint_path, load_cfg_from_registry, parse_env_cfg
 from omni.isaac.lab_tasks.utils.wrappers.sb3 import Sb3VecEnvWrapper, process_sb3_cfg
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from flat_env_cfg import AnymalDFlatEnvCfg
-
-
-# def main():
-#     """Play with stable-baselines agent."""
-#     # parse configuration
-#     # env_cfg = parse_env_cfg(
-#     #     args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric
-#     # )
-#     env_cfg = AnymalDFlatEnvCfg()
-#     # agent_cfg = load_cfg_from_registry("AnymalDFlatEnvCfg", "sb3_cfg_entry_point")
-#     agent_cfg = {
-#         "gamma": 0.99,  # Discount factor
-#         "learning_rate": 3e-4,  # Learning rate
-#         "policy": "MlpPolicy",  # Policy type, adjust as necessary
-#         "normalize_input": True,  # Normalization settings
-#         "clip_obs": 10.0,
-#         # Add other relevant parameters as needed
-#     }
-
-#     # directory for logging into
-#     log_root_path = os.path.join("logs", "sb3", "AnymalDFlatEnv")
-#     log_root_path = os.path.abspath(log_root_path)
-#     # check checkpoint is valid
-#     if args_cli.checkpoint is None:
-#         if args_cli.use_last_checkpoint:
-#             checkpoint = "model_.*.zip"
-#         else:
-#             checkpoint = "model.zip"
-#         checkpoint_path = get_checkpoint_path(log_root_path, ".*", checkpoint)
-#     else:
-#         checkpoint_path = args_cli.checkpoint
-#     log_dir = os.path.dirname(checkpoint_path)
-
-#     # post-process agent configuration
-#     agent_cfg = process_sb3_cfg(agent_cfg)
-
-#     # create isaac environment
-#     # Create the environment instance
-#     env = ManagerBasedRLEnv(cfg=env_cfg)
-
-#     # Wrap the environment for Stable Baselines
-#     env = Sb3VecEnvWrapper(env)
-#     # env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
-    
-#     # wrap for video recording
-#     if args_cli.video:
-#         video_kwargs = {
-#             "video_folder": os.path.join(log_dir, "videos", "play"),
-#             "step_trigger": lambda step: step == 0,
-#             "video_length": args_cli.video_length,
-#             "disable_logger": True,
-#         }
-#         print("[INFO] Recording videos during training.")
-#         print_dict(video_kwargs, nesting=4)
-#         env = gym.wrappers.RecordVideo(env, **video_kwargs)
-
-#     # wrap around environment for stable baselines
-#     # env = Sb3VecEnvWrapper(env)
-
-#     # normalize environment (if needed)
-#     if "normalize_input" in agent_cfg:
-#         env = VecNormalize(
-#             env,
-#             training=True,
-#             norm_obs="normalize_input" in agent_cfg and agent_cfg.pop("normalize_input"),
-#             norm_reward="normalize_value" in agent_cfg and agent_cfg.pop("normalize_value"),
-#             clip_obs="clip_obs" in agent_cfg and agent_cfg.pop("clip_obs"),
-#             gamma=agent_cfg["gamma"],
-#             clip_reward=np.inf,
-#         )
-
-#     # # create agent from stable baselines
-#     # print(f"Loading checkpoint from: {checkpoint_path}")
-#     # agent = PPO.load(checkpoint_path, env, print_system_info=True)
-
-#     # Create agent from Stable Baselines with checkpoint handling
-#     print(f"Loading checkpoint from: {checkpoint_path}")
-#     with open(checkpoint_path, "rb") as f:
-#         agent = PPO.load(f, env=env, weights_only=True)
-
-#     # reset environment
-#     obs = env.reset()
-#     timestep = 0
-#     # simulate environment
-#     while simulation_app.is_running():
-#         # run everything in inference mode
-#         with torch.inference_mode():
-#             # agent stepping
-#             actions, _ = agent.predict(obs, deterministic=True)
-#             # env stepping
-#             obs, _, _, _ = env.step(actions)
-#         if args_cli.video:
-#             timestep += 1
-#             # Exit the play loop after recording one video
-#             if timestep == args_cli.video_length:
-#                 break
-
-#     # close the simulator
-#     env.close()
-
-
-# if __name__ == "__main__":
-#     # run the main function
-#     main()
-#     # close sim app
-#     simulation_app.close()
-
-####################################################################################################
-
-# def main():
-#     """Play with a stable-baselines agent."""
-#     # Configure environment and agent
-#     env_cfg = AnymalDFlatEnvCfg()
-#     agent_cfg = {
-#         "gamma": 0.99,
-#         "learning_rate": 3e-4,
-#         "policy": "MlpPolicy",
-#         "normalize_input": True,
-#         "clip_obs": 10.0,
-#     }
-    
-#     # Define logging paths and checkpoint paths
-#     log_root_path = os.path.abspath(os.path.join("logs", "sb3", "AnymalDFlatEnv"))
-#     if args_cli.checkpoint is None:
-#         checkpoint = "model_.*.zip" if args_cli.use_last_checkpoint else "model.zip"
-#         checkpoint_path = get_checkpoint_path(log_root_path, ".*", checkpoint)
-#     else:
-#         checkpoint_path = args_cli.checkpoint
-#     log_dir = os.path.dirname(checkpoint_path)
-
-#     # Process agent configuration
-#     agent_cfg = process_sb3_cfg(agent_cfg)
-
-#     # Create the Isaac environment
-#     env = ManagerBasedRLEnv(cfg=env_cfg)
-#     env = Sb3VecEnvWrapper(env)  # Wrap for SB3 compatibility
-
-#     # Add video recording if requested
-#     if args_cli.video:
-#         video_kwargs = {
-#             "video_folder": os.path.join(log_dir, "videos", "play"),
-#             "step_trigger": lambda step: step == 0,
-#             "video_length": args_cli.video_length,
-#             "disable_logger": True,
-#         }
-#         print("[INFO] Recording videos during training.")
-#         print_dict(video_kwargs, nesting=4)
-#         env = gym.wrappers.RecordVideo(env, **video_kwargs)
-
-#     # Normalize environment (if needed)
-#     if "normalize_input" in agent_cfg:
-#         env = VecNormalize(
-#             env,
-#             training=True,
-#             norm_obs="normalize_input" in agent_cfg and agent_cfg.pop("normalize_input"),
-#             norm_reward="normalize_value" in agent_cfg and agent_cfg.pop("normalize_value"),
-#             clip_obs="clip_obs" in agent_cfg and agent_cfg.pop("clip_obs"),
-#             gamma=agent_cfg["gamma"],
-#             clip_reward=np.inf,
-#         )
-
-#     # Load the agent with explicit torch.load call
-#     print(f"Loading checkpoint from: {checkpoint_path}")
-#     checkpoint_data = torch.load(checkpoint_path, weights_only=True)
-#     agent = PPO.load(checkpoint_data, env=env)
-    
-#     # For RNN-based policy, flatten parameters if necessary
-#     if hasattr(agent.policy, 'rnn'):
-#         agent.policy.rnn.flatten_parameters()
-
-#     # Reset environment and start simulation loop
-#     obs = env.reset()
-#     timestep = 0
-#     while simulation_app.is_running():
-#         with torch.inference_mode():
-#             actions, _ = agent.predict(obs, deterministic=True)
-#             obs, _, _, _ = env.step(actions)
-#         if args_cli.video:
-#             timestep += 1
-#             if timestep == args_cli.video_length:
-#                 break
-
-#     # Close environment and simulation app
-#     env.close()
-
-# if __name__ == "__main__":
-#     main()
-#     simulation_app.close()
 
+# Import the AnymalDFlatEnvCfg from your custom task file
+from flat_env_cfg import AnymalDFlatEnvCfg
 
-######################################################################################################
+##############################################################
+#
+#           Main Function
+#
+##############################################################
 
 def main():
     """Play with a stable-baselines agent."""
     # Configure environment and agent
     env_cfg = AnymalDFlatEnvCfg()
+    env_cfg.scene.num_envs = args_cli.num_envs
+
+    # agent_cfg = {
+    #     "gamma": 0.99,
+    #     "learning_rate": 3e-4,
+    #     "policy": "MlpPolicy",
+    #     "normalize_input": True,
+    #     "clip_obs": 10.0,
+    # }
+
     agent_cfg = {
-        "gamma": 0.99,
-        "learning_rate": 3e-4,
-        "policy": "MlpPolicy",
+        'seed': 42, 
+        'n_timesteps': 1000000.0, 
+        'policy': 'MlpPolicy', 
+        'n_steps': 16, 
+        'batch_size': 4096, 
+        'gae_lambda': 0.95, 
+        'gamma': 0.99, 
+        'n_epochs': 20, 
+        'ent_coef': 0.01, 
+        'learning_rate': 0.0003, 
+        'clip_range': 0.2, 
+        'policy_kwargs': 
+        'dict( activation_fn=nn.ELU, net_arch=[32, 32], squash_output=False, )', 
+        'vf_coef': 1.0, 
+        'max_grad_norm': 1.0,
         "normalize_input": True,
-        "clip_obs": 10.0,
+        "clip_obs": 10.0, 
+        'device': 'cuda:0'
     }
     
-    # # Define logging paths and checkpoint paths
-    # log_root_path = os.path.abspath(os.path.join("logs", "sb3", "AnymalDFlatEnv"))
-    # if args_cli.checkpoint is None:
-    #     checkpoint = "model_.*.zip" if args_cli.use_last_checkpoint else "model.zip"
-    #     checkpoint_path = get_checkpoint_path(log_root_path, ".*", checkpoint)
-    # else:
-    #     checkpoint_path = args_cli.checkpoint
-    # log_dir = os.path.dirname(checkpoint_path)
-    # directory for logging into
+    # Define logging paths and checkpoint paths
     log_root_path = os.path.join("logs", "sb3", "AnymalDFlatEnv")
     log_root_path = os.path.abspath(log_root_path)
-    # check checkpoint is valid
+    # Check if a checkpoint path is provided
     if args_cli.checkpoint is None:
         if args_cli.use_last_checkpoint:
             checkpoint = "model_.*.zip"
@@ -305,36 +153,28 @@ def main():
         env = VecNormalize(
             env,
             training=True,
-            norm_obs="normalize_input" in agent_cfg and agent_cfg.pop("normalize_input"),
-            norm_reward="normalize_value" in agent_cfg and agent_cfg.pop("normalize_value"),
-            clip_obs="clip_obs" in agent_cfg and agent_cfg.pop("clip_obs"),
+            norm_obs=agent_cfg.pop("normalize_input", False),
+            norm_reward=agent_cfg.pop("normalize_value", False),
+            clip_obs=agent_cfg.pop("clip_obs", 10.0),
             gamma=agent_cfg["gamma"],
             clip_reward=np.inf,
         )
 
-    # Load the agent from Stable Baselines without torch.load
+    # Load the agent directly with PPO.load()
     print(f"Loading checkpoint from: {checkpoint_path}")
     
-    import warnings
-    warnings.filterwarnings("ignore", category=FutureWarning, message="You are using `torch.load` with `weights_only=False`")
-
-    agent = PPO.load(checkpoint_path, env=env, print_system_info=True)
+    # Loading the checkpoint without using torch.load
+    agent = PPO.load(checkpoint_path, env=env)
 
     print(f"Reset environment and start simulation loop")
     # Reset environment and start simulation loop
     obs = env.reset()
     timestep = 0
-    print(f"simulation_app")
     while simulation_app.is_running():
-        print(f"simulation_apppppppppppp")
         with torch.inference_mode():
-            print(f"torch.inference_mode")
             actions, _ = agent.predict(obs, deterministic=True)
-            print(f"11111111")
             obs, _, _, _ = env.step(actions)
-            print(f"2222222")
         if args_cli.video:
-            print(f"args_cli.video")
             timestep += 1
             if timestep == args_cli.video_length:
                 break
diff --git a/exts/extensions/extensions/tutorials/manager_based/sb3/train_sb3.py b/exts/extensions/extensions/tutorials/manager_based/sb3/train_sb3.py
deleted file mode 100644
index 9c3ca1e..0000000
--- a/exts/extensions/extensions/tutorials/manager_based/sb3/train_sb3.py
+++ /dev/null
@@ -1,158 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Script to train RL agent with Stable Baselines3.
-
-Since Stable-Baselines3 does not support buffers living on GPU directly,
-we recommend using smaller number of environments. Otherwise,
-there will be significant overhead in GPU->CPU transfer.
-"""
-
-"""Launch Isaac Sim Simulator first."""
-
-import argparse
-import sys
-
-from omni.isaac.lab.app import AppLauncher
-
-# add argparse arguments
-parser = argparse.ArgumentParser(description="Train an RL agent with Stable-Baselines3.")
-parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
-parser.add_argument("--video_length", type=int, default=200, help="Length of the recorded video (in steps).")
-parser.add_argument("--video_interval", type=int, default=2000, help="Interval between video recordings (in steps).")
-parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
-parser.add_argument("--task", type=str, default="manager_based/flat_env_cfg.py", help="Name of the task.")
-parser.add_argument("--seed", type=int, default=None, help="Seed used for the environment")
-parser.add_argument("--max_iterations", type=int, default=None, help="RL Policy training iterations.")
-# append AppLauncher cli args
-AppLauncher.add_app_launcher_args(parser)
-# parse the arguments
-args_cli, hydra_args = parser.parse_known_args()
-# always enable cameras to record video
-if args_cli.video:
-    args_cli.enable_cameras = True
-
-# clear out sys.argv for Hydra
-sys.argv = [sys.argv[0]] + hydra_args
-
-# launch omniverse app
-app_launcher = AppLauncher(args_cli)
-simulation_app = app_launcher.app
-
-"""Rest everything follows."""
-
-import gymnasium as gym
-import numpy as np
-import os
-import random
-from datetime import datetime
-
-from stable_baselines3 import PPO
-from stable_baselines3.common.callbacks import CheckpointCallback
-from stable_baselines3.common.logger import configure
-from stable_baselines3.common.vec_env import VecNormalize
-
-from omni.isaac.lab.envs import (
-    DirectMARLEnv,
-    DirectMARLEnvCfg,
-    DirectRLEnvCfg,
-    ManagerBasedRLEnvCfg,
-    multi_agent_to_single_agent,
-)
-from omni.isaac.lab.utils.dict import print_dict
-from omni.isaac.lab.utils.io import dump_pickle, dump_yaml
-
-import omni.isaac.lab_tasks  # noqa: F401
-from omni.isaac.lab_tasks.utils.hydra import hydra_task_config
-from omni.isaac.lab_tasks.utils.wrappers.sb3 import Sb3VecEnvWrapper, process_sb3_cfg
-
-
-@hydra_task_config(args_cli.task, "sb3_cfg_entry_point")
-def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: dict):
-    """Train with stable-baselines agent."""
-    # randomly sample a seed if seed = -1
-    if args_cli.seed == -1:
-        args_cli.seed = random.randint(0, 10000)
-
-    # override configurations with non-hydra CLI arguments
-    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs
-    agent_cfg["seed"] = args_cli.seed if args_cli.seed is not None else agent_cfg["seed"]
-    # max iterations for training
-    if args_cli.max_iterations is not None:
-        agent_cfg["n_timesteps"] = args_cli.max_iterations * agent_cfg["n_steps"] * env_cfg.scene.num_envs
-
-    # set the environment seed
-    # note: certain randomizations occur in the environment initialization so we set the seed here
-    env_cfg.seed = agent_cfg["seed"]
-    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device
-
-    # directory for logging into
-    log_dir = os.path.join("logs", "sb3", args_cli.task, datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
-    # dump the configuration into log-directory
-    dump_yaml(os.path.join(log_dir, "params", "env.yaml"), env_cfg)
-    dump_yaml(os.path.join(log_dir, "params", "agent.yaml"), agent_cfg)
-    dump_pickle(os.path.join(log_dir, "params", "env.pkl"), env_cfg)
-    dump_pickle(os.path.join(log_dir, "params", "agent.pkl"), agent_cfg)
-
-    # post-process agent configuration
-    agent_cfg = process_sb3_cfg(agent_cfg)
-    # read configurations about the agent-training
-    policy_arch = agent_cfg.pop("policy")
-    n_timesteps = agent_cfg.pop("n_timesteps")
-
-    # create isaac environment
-    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
-    # wrap for video recording
-    if args_cli.video:
-        video_kwargs = {
-            "video_folder": os.path.join(log_dir, "videos", "train"),
-            "step_trigger": lambda step: step % args_cli.video_interval == 0,
-            "video_length": args_cli.video_length,
-            "disable_logger": True,
-        }
-        print("[INFO] Recording videos during training.")
-        print_dict(video_kwargs, nesting=4)
-        env = gym.wrappers.RecordVideo(env, **video_kwargs)
-
-    # convert to single-agent instance if required by the RL algorithm
-    if isinstance(env.unwrapped, DirectMARLEnv):
-        env = multi_agent_to_single_agent(env)
-
-    # wrap around environment for stable baselines
-    env = Sb3VecEnvWrapper(env)
-
-    if "normalize_input" in agent_cfg:
-        env = VecNormalize(
-            env,
-            training=True,
-            norm_obs="normalize_input" in agent_cfg and agent_cfg.pop("normalize_input"),
-            norm_reward="normalize_value" in agent_cfg and agent_cfg.pop("normalize_value"),
-            clip_obs="clip_obs" in agent_cfg and agent_cfg.pop("clip_obs"),
-            gamma=agent_cfg["gamma"],
-            clip_reward=np.inf,
-        )
-
-    # create agent from stable baselines
-    agent = PPO(policy_arch, env, verbose=1, **agent_cfg)
-    # configure the logger
-    new_logger = configure(log_dir, ["stdout", "tensorboard"])
-    agent.set_logger(new_logger)
-
-    # callbacks for agent
-    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=log_dir, name_prefix="model", verbose=2)
-    # train the agent
-    agent.learn(total_timesteps=n_timesteps, callback=checkpoint_callback)
-    # save the final model
-    agent.save(os.path.join(log_dir, "model"))
-
-    # close the simulator
-    env.close()
-
-
-if __name__ == "__main__":
-    # run the main function
-    main()
-    # close sim app
-    simulation_app.close()
\ No newline at end of file
diff --git a/exts/extensions/extensions/tutorials/manager_based/sb4/play.py b/exts/extensions/extensions/tutorials/manager_based/sb4/play.py
deleted file mode 100644
index e269089..0000000
--- a/exts/extensions/extensions/tutorials/manager_based/sb4/play.py
+++ /dev/null
@@ -1,140 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Script to play a checkpoint if an RL agent from Stable-Baselines3."""
-
-"""Launch Isaac Sim Simulator first."""
-
-import argparse
-
-from omni.isaac.lab.app import AppLauncher
-
-# add argparse arguments
-parser = argparse.ArgumentParser(description="Play a checkpoint of an RL agent from Stable-Baselines3.")
-parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
-parser.add_argument("--video_length", type=int, default=200, help="Length of the recorded video (in steps).")
-parser.add_argument(
-    "--disable_fabric", action="store_true", default=False, help="Disable fabric and use USD I/O operations."
-)
-parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
-parser.add_argument("--task", type=str, default=None, help="Name of the task.")
-parser.add_argument("--checkpoint", type=str, default=None, help="Path to model checkpoint.")
-parser.add_argument(
-    "--use_last_checkpoint",
-    action="store_true",
-    help="When no checkpoint provided, use the last saved model. Otherwise use the best saved model.",
-)
-# append AppLauncher cli args
-AppLauncher.add_app_launcher_args(parser)
-# parse the arguments
-args_cli = parser.parse_args()
-# always enable cameras to record video
-if args_cli.video:
-    args_cli.enable_cameras = True
-
-# launch omniverse app
-app_launcher = AppLauncher(args_cli)
-simulation_app = app_launcher.app
-
-"""Rest everything follows."""
-
-import gymnasium as gym
-import numpy as np
-import os
-import torch
-
-from stable_baselines3 import PPO
-from stable_baselines3.common.vec_env import VecNormalize
-
-from omni.isaac.lab.utils.dict import print_dict
-
-import omni.isaac.lab_tasks  # noqa: F401
-from omni.isaac.lab_tasks.utils.parse_cfg import get_checkpoint_path, load_cfg_from_registry, parse_env_cfg
-from omni.isaac.lab_tasks.utils.wrappers.sb3 import Sb3VecEnvWrapper, process_sb3_cfg
-
-
-def main():
-    """Play with stable-baselines agent."""
-    # parse configuration
-    env_cfg = parse_env_cfg(
-        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric
-    )
-    agent_cfg = load_cfg_from_registry(args_cli.task, "sb3_cfg_entry_point")
-
-    # directory for logging into
-    log_root_path = os.path.join("logs", "sb3", args_cli.task)
-    log_root_path = os.path.abspath(log_root_path)
-    # check checkpoint is valid
-    if args_cli.checkpoint is None:
-        if args_cli.use_last_checkpoint:
-            checkpoint = "model_.*.zip"
-        else:
-            checkpoint = "model.zip"
-        checkpoint_path = get_checkpoint_path(log_root_path, ".*", checkpoint)
-    else:
-        checkpoint_path = args_cli.checkpoint
-    log_dir = os.path.dirname(checkpoint_path)
-
-    # post-process agent configuration
-    agent_cfg = process_sb3_cfg(agent_cfg)
-
-    # create isaac environment
-    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
-    # wrap for video recording
-    if args_cli.video:
-        video_kwargs = {
-            "video_folder": os.path.join(log_dir, "videos", "play"),
-            "step_trigger": lambda step: step == 0,
-            "video_length": args_cli.video_length,
-            "disable_logger": True,
-        }
-        print("[INFO] Recording videos during training.")
-        print_dict(video_kwargs, nesting=4)
-        env = gym.wrappers.RecordVideo(env, **video_kwargs)
-    # wrap around environment for stable baselines
-    env = Sb3VecEnvWrapper(env)
-
-    # normalize environment (if needed)
-    if "normalize_input" in agent_cfg:
-        env = VecNormalize(
-            env,
-            training=True,
-            norm_obs="normalize_input" in agent_cfg and agent_cfg.pop("normalize_input"),
-            norm_reward="normalize_value" in agent_cfg and agent_cfg.pop("normalize_value"),
-            clip_obs="clip_obs" in agent_cfg and agent_cfg.pop("clip_obs"),
-            gamma=agent_cfg["gamma"],
-            clip_reward=np.inf,
-        )
-
-    # create agent from stable baselines
-    print(f"Loading checkpoint from: {checkpoint_path}")
-    agent = PPO.load(checkpoint_path, env, print_system_info=True)
-
-    # reset environment
-    obs = env.reset()
-    timestep = 0
-    # simulate environment
-    while simulation_app.is_running():
-        # run everything in inference mode
-        with torch.inference_mode():
-            # agent stepping
-            actions, _ = agent.predict(obs, deterministic=True)
-            # env stepping
-            obs, _, _, _ = env.step(actions)
-        if args_cli.video:
-            timestep += 1
-            # Exit the play loop after recording one video
-            if timestep == args_cli.video_length:
-                break
-
-    # close the simulator
-    env.close()
-
-
-if __name__ == "__main__":
-    # run the main function
-    main()
-    # close sim app
-    simulation_app.close()
diff --git a/exts/extensions/extensions/tutorials/manager_based/sb4/train.py b/exts/extensions/extensions/tutorials/manager_based/sb4/train.py
deleted file mode 100644
index 1ce8062..0000000
--- a/exts/extensions/extensions/tutorials/manager_based/sb4/train.py
+++ /dev/null
@@ -1,158 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Script to train RL agent with Stable Baselines3.
-
-Since Stable-Baselines3 does not support buffers living on GPU directly,
-we recommend using smaller number of environments. Otherwise,
-there will be significant overhead in GPU->CPU transfer.
-"""
-
-"""Launch Isaac Sim Simulator first."""
-
-import argparse
-import sys
-
-from omni.isaac.lab.app import AppLauncher
-
-# add argparse arguments
-parser = argparse.ArgumentParser(description="Train an RL agent with Stable-Baselines3.")
-parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
-parser.add_argument("--video_length", type=int, default=200, help="Length of the recorded video (in steps).")
-parser.add_argument("--video_interval", type=int, default=2000, help="Interval between video recordings (in steps).")
-parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
-parser.add_argument("--task", type=str, default=None, help="Name of the task.")
-parser.add_argument("--seed", type=int, default=None, help="Seed used for the environment")
-parser.add_argument("--max_iterations", type=int, default=None, help="RL Policy training iterations.")
-# append AppLauncher cli args
-AppLauncher.add_app_launcher_args(parser)
-# parse the arguments
-args_cli, hydra_args = parser.parse_known_args()
-# always enable cameras to record video
-if args_cli.video:
-    args_cli.enable_cameras = True
-
-# clear out sys.argv for Hydra
-sys.argv = [sys.argv[0]] + hydra_args
-
-# launch omniverse app
-app_launcher = AppLauncher(args_cli)
-simulation_app = app_launcher.app
-
-"""Rest everything follows."""
-
-import gymnasium as gym
-import numpy as np
-import os
-import random
-from datetime import datetime
-
-from stable_baselines3 import PPO
-from stable_baselines3.common.callbacks import CheckpointCallback
-from stable_baselines3.common.logger import configure
-from stable_baselines3.common.vec_env import VecNormalize
-
-from omni.isaac.lab.envs import (
-    DirectMARLEnv,
-    DirectMARLEnvCfg,
-    DirectRLEnvCfg,
-    ManagerBasedRLEnvCfg,
-    multi_agent_to_single_agent,
-)
-from omni.isaac.lab.utils.dict import print_dict
-from omni.isaac.lab.utils.io import dump_pickle, dump_yaml
-
-import omni.isaac.lab_tasks  # noqa: F401
-from omni.isaac.lab_tasks.utils.hydra import hydra_task_config
-from omni.isaac.lab_tasks.utils.wrappers.sb3 import Sb3VecEnvWrapper, process_sb3_cfg
-
-
-@hydra_task_config(args_cli.task, "sb3_cfg_entry_point")
-def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: dict):
-    """Train with stable-baselines agent."""
-    # randomly sample a seed if seed = -1
-    if args_cli.seed == -1:
-        args_cli.seed = random.randint(0, 10000)
-
-    # override configurations with non-hydra CLI arguments
-    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs
-    agent_cfg["seed"] = args_cli.seed if args_cli.seed is not None else agent_cfg["seed"]
-    # max iterations for training
-    if args_cli.max_iterations is not None:
-        agent_cfg["n_timesteps"] = args_cli.max_iterations * agent_cfg["n_steps"] * env_cfg.scene.num_envs
-
-    # set the environment seed
-    # note: certain randomizations occur in the environment initialization so we set the seed here
-    env_cfg.seed = agent_cfg["seed"]
-    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device
-
-    # directory for logging into
-    log_dir = os.path.join("logs", "sb3", args_cli.task, datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
-    # dump the configuration into log-directory
-    dump_yaml(os.path.join(log_dir, "params", "env.yaml"), env_cfg)
-    dump_yaml(os.path.join(log_dir, "params", "agent.yaml"), agent_cfg)
-    dump_pickle(os.path.join(log_dir, "params", "env.pkl"), env_cfg)
-    dump_pickle(os.path.join(log_dir, "params", "agent.pkl"), agent_cfg)
-
-    # post-process agent configuration
-    agent_cfg = process_sb3_cfg(agent_cfg)
-    # read configurations about the agent-training
-    policy_arch = agent_cfg.pop("policy")
-    n_timesteps = agent_cfg.pop("n_timesteps")
-
-    # create isaac environment
-    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
-    # wrap for video recording
-    if args_cli.video:
-        video_kwargs = {
-            "video_folder": os.path.join(log_dir, "videos", "train"),
-            "step_trigger": lambda step: step % args_cli.video_interval == 0,
-            "video_length": args_cli.video_length,
-            "disable_logger": True,
-        }
-        print("[INFO] Recording videos during training.")
-        print_dict(video_kwargs, nesting=4)
-        env = gym.wrappers.RecordVideo(env, **video_kwargs)
-
-    # convert to single-agent instance if required by the RL algorithm
-    if isinstance(env.unwrapped, DirectMARLEnv):
-        env = multi_agent_to_single_agent(env)
-
-    # wrap around environment for stable baselines
-    env = Sb3VecEnvWrapper(env)
-
-    if "normalize_input" in agent_cfg:
-        env = VecNormalize(
-            env,
-            training=True,
-            norm_obs="normalize_input" in agent_cfg and agent_cfg.pop("normalize_input"),
-            norm_reward="normalize_value" in agent_cfg and agent_cfg.pop("normalize_value"),
-            clip_obs="clip_obs" in agent_cfg and agent_cfg.pop("clip_obs"),
-            gamma=agent_cfg["gamma"],
-            clip_reward=np.inf,
-        )
-
-    # create agent from stable baselines
-    agent = PPO(policy_arch, env, verbose=1, **agent_cfg)
-    # configure the logger
-    new_logger = configure(log_dir, ["stdout", "tensorboard"])
-    agent.set_logger(new_logger)
-
-    # callbacks for agent
-    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=log_dir, name_prefix="model", verbose=2)
-    # train the agent
-    agent.learn(total_timesteps=n_timesteps, callback=checkpoint_callback)
-    # save the final model
-    agent.save(os.path.join(log_dir, "model"))
-
-    # close the simulator
-    env.close()
-
-
-if __name__ == "__main__":
-    # run the main function
-    main()
-    # close sim app
-    simulation_app.close()
diff --git a/exts/extensions/extensions/tutorials/manager_based/train.py b/exts/extensions/extensions/tutorials/manager_based/train.py
index cb5fe6d..e4d1dcf 100644
--- a/exts/extensions/extensions/tutorials/manager_based/train.py
+++ b/exts/extensions/extensions/tutorials/manager_based/train.py
@@ -1,13 +1,19 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
+"""
+    This script use for interacting with an articulation for quadruped robot in Isaac Sim.
+
+    #   Usage
+    python exts/extensions/extensions/tutorials/manager_based/train.py --headless
+"""
 
 """Script to train RL agent with Stable Baselines3 using AnymalDFlatEnvCfg."""
 
+##############################################################
+#
+#           Setting argparse arguments 
+#
+##############################################################
+
 import argparse
-import sys
-from omni.isaac.lab.app import AppLauncher
 
 # Add argparse arguments
 parser = argparse.ArgumentParser(description="Train an RL agent with Stable-Baselines3.")
@@ -17,39 +23,58 @@ parser.add_argument("--video_interval", type=int, default=2000, help="Interval b
 parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
 parser.add_argument("--seed", type=int, default=None, help="Seed used for the environment")
 parser.add_argument("--max_iterations", type=int, default=None, help="RL Policy training iterations.")
+
+##############################################################
+#
+#           Launch Isaac Sim Simulator first.
+#
+##############################################################
+
+from omni.isaac.lab.app import AppLauncher
+
+# append AppLauncher cli args
 AppLauncher.add_app_launcher_args(parser)
+# parse the arguments
 args_cli = parser.parse_args()
+# always enable cameras to record video
 if args_cli.video:
     args_cli.enable_cameras = True
 
+# launch omniverse app
 app_launcher = AppLauncher(args_cli)
 simulation_app = app_launcher.app
 
+##############################################################
+#
+#           Import the library for run play
+#
+##############################################################
+
 import gymnasium as gym
 import numpy as np
 import os
 import random
 from datetime import datetime
+
 from stable_baselines3 import PPO
 from stable_baselines3.common.callbacks import CheckpointCallback
 from stable_baselines3.common.logger import configure
 from stable_baselines3.common.vec_env import VecNormalize
 
-from omni.isaac.lab.envs import (
-    DirectMARLEnv,
-    DirectMARLEnvCfg,
-    DirectRLEnvCfg,
-    ManagerBasedRLEnvCfg,
-    multi_agent_to_single_agent,
-)
 from omni.isaac.lab.utils.dict import print_dict
 from omni.isaac.lab.utils.io import dump_pickle, dump_yaml
 from omni.isaac.lab.envs import ManagerBasedRLEnv
 
+from omni.isaac.lab_tasks.utils.wrappers.sb3 import Sb3VecEnvWrapper, process_sb3_cfg
+
 # Import the AnymalDFlatEnvCfg from your custom task file
 from flat_env_cfg import AnymalDFlatEnvCfg
-from omni.isaac.lab_tasks.utils.wrappers.sb3 import Sb3VecEnvWrapper, process_sb3_cfg
 
+##############################################################
+#
+#           Main Function
+#
+##############################################################
 
 def main():
     """Train with stable-baselines agent using AnymalDFlatEnvCfg."""